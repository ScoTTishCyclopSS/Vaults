## Obsah:
1. [[#1. ParalelnÃ­ systÃ©my/vÃ½poÄty|ParalelnÃ­ systÃ©my/vÃ½poÄty]]
- HardwarovÃ¡ podpora pro paralelnÃ­ vÃ½poÄty: (super)skalÃ¡rnÃ­ architektury, pipelining, spekulativnÃ­ vyhodnocovÃ¡nÃ­, vektorovÃ© instrukce, vlÃ¡kna, procesy, GPGPU. Hierarchie cache pamÄ›tÃ­.
- Komplikace v paralelnÃ­m programovÃ¡nÃ­: soubÄ›h (race condition), uvÃ¡znutÃ­ (deadlock), iluze sdÃ­lenÃ­ (false sharing).
- Podpora paralelnÃ­ho programovÃ¡nÃ­ v C a C++: pthreads, thread, jthread, atomic, mutex, lock_guard.
- Podpora paralelnÃ­ho programovÃ¡nÃ­ v OpenMP: sÃ©riovÄ›-paralelnÃ­ model uspoÅ™Ã¡dÃ¡nÃ­ vlÃ¡ken (fork-join), paralelizovatelnÃ¡ Ãºloha (task region), rÅ¯znÃ© implementace specifikace. Direktivy parallel, for, section, task, barrier, critical, atomic.
- Techniky dekompozice programu: statickÃ© a paralelnÃ­ rozdÄ›lenÃ­ prÃ¡ce. Threadpool a fronta ÃºkolÅ¯. BalancovÃ¡nÃ­ a zÃ¡vislosti (dependencies).
- Techniky dekompozice programu na pÅ™Ã­kladech z Å™azenÃ­: quick sort, merge sort.
- Techniky dekompozice programu na pÅ™Ã­kladech z numerickÃ© lineÃ¡rnÃ­ algebry a strojovÃ©ho uÄenÃ­: nÃ¡sobenÃ­ matice vektorem, nÃ¡sobenÃ­ dvou matic, Å™eÅ¡enÃ­ systÃ©mu lineÃ¡rnÃ­ch rovnic.
2. [[#2. DistribuovanÃ© vÃ½poÄty/systÃ©my|DistribuovanÃ© vÃ½poÄty/systÃ©my]]
- Ãšvod do distribuovanÃ½ch systÃ©mÅ¯ (DS). Charakteristiky DS. ÄŒas a typy selhÃ¡nÃ­ v DS. 
- Detekce selhÃ¡nÃ­ v DS. Detektory selhÃ¡nÃ­ a jejich vlastnosti. 
- ÄŒas a kauzalita v DS. UspoÅ™Ã¡dÃ¡nÃ­ udÃ¡lostÃ­ v DS. FyzickÃ© hodiny a jejich synchronizace. LogickÃ© hodiny a jejich synchronizace. 
- GlobÃ¡lnÃ­ stav v DS a jeho vÃ½poÄet. Å˜ez distribuovanÃ©ho vÃ½poÄtu. Algoritmus pro distribuovanÃ½ globÃ¡lnÃ­ snapshot. StabilnÃ­ vlastnosti DS. 
- VzÃ¡jemnÃ© vylouÄenÃ­ procesÅ¯ v DS. Algoritmy pro vylouÄenÃ­ procesÅ¯ a jejich vlastnosti. 
- Volba lÃ­dra v DS. Algoritmy pro volbu lÃ­dra a jejich vlastnosti. 
- Konsensus v DS. FLP teorÃ©m. Algoritmy pro distribuovanÃ½ konsensus.

---

## 1. ParalelnÃ­ systÃ©my/vÃ½poÄty

### HardwarovÃ¡ podpora pro paralelnÃ­ vÃ½poÄty

#### (Super)SkalÃ¡rnÃ­ architektury

UmoÅ¾ÅˆujÃ­ provÃ¡dÄ›t vÃ­ce instrukcÃ­ najednou. Procesor rozdÄ›luje instrukce na menÅ¡Ã­ ÄÃ¡sti a provÃ¡dÃ­ je paralelnÄ›, coÅ¾ zvyÅ¡uje vÃ½kon. Architektura zahrnuje vÃ­ce vykonÃ¡vacÃ­ch jednotek (ALU), kterÃ© mohou pracovat nezÃ¡visle na sobÄ›.

EX:
UvaÅ¾me cyklus:

~~~
for (i = 0; i < 1000; i++) 
	z[i] = x[i] + y[i];
~~~
jedna jednotka ALU mÅ¯Å¾e poÄÃ­tat z\[0], druhÃ¡ z\[1] atd.

#### Pipelining (model paralelismu)

UmoÅ¾Åˆuje **Paralelizace na Ãºrovni instrukcÃ­ (ILP)**. ILP rozdÄ›luje vykonÃ¡vÃ¡nÃ­ instrukcÃ­ do nÄ›kolika fÃ¡zÃ­. KaÅ¾dÃ¡ fÃ¡ze zpracovÃ¡vÃ¡ jinou operaci, coÅ¾ umoÅ¾Åˆuje, aby se instrukce provÃ¡dÄ›ly sekvenÄnÄ›, ale zÃ¡roveÅˆ paralelnÄ›. 

==KaÅ¾dÃ© vlÃ¡kno provede nÄ›jakou prÃ¡ci nad daty a poÅ¡le vÃ½sledek dalÅ¡Ã­mu.==

EX:
Chceme seÄÃ­st 2 vektory reÃ¡lnÃ½ch ÄÃ­sel (float \[1000])
1 souÄet â€“ 7 operacÃ­:
- NaÄtenÃ­ (fetch)
- PorovnÃ¡nÃ­ exponentÅ¯
- Posun
- SouÄet
- Normalizace
- ZaokrouhlenÃ­
- UloÅ¾enÃ­ vÃ½sledku

Bez ILP -> 7x1000x (Äas 1 operace = 1ns) -> 7000 ns
S ILP (7 jednotek) -> 1000 ns

#### SpekulativnÃ­ vyhodnocovÃ¡nÃ­

#todo

#### VektorovÃ© instrukce

Tyto instrukce umoÅ¾ÅˆujÃ­ provÃ¡dÄ›t stejnou operaci na vÃ­ce datech souÄasnÄ›, coÅ¾ zvyÅ¡uje vÃ½kon a zpracovÃ¡vÃ¡vÃ¡ vÃ­ce ÃºkolÅ¯ za jedno kolo instrukcÃ­. VektorovÃ© instrukce jsou zvlÃ¡Å¡tÄ› uÅ¾iteÄnÃ© pro aplikace, kterÃ© pracujÃ­ s velkÃ½mi mnoÅ¾stvÃ­mi dat najednou (e.g. zpracovÃ¡nÃ­ obrazu).



EX:
VektorovÃ© instrukce jsou souÄÃ¡stÃ­ architektury procesoru, ale my, programÃ¡toÅ™i, k nim mÃ¡me pÅ™Ã­stup prostÅ™ednictvÃ­m knihoven, jako je nÃ­Å¾e uvedenÃ½ kÃ³d.  

~~~
#include <immintrin.h>

void vectorizedSum(float *a, float *b, float *result, int size) {
    for (int i = 0; i < size; i += 8) {
        __m256 va = _mm256_load_ps(&a[i]);
        __m256 vb = _mm256_load_ps(&b[i]);
        __m256 vresult = _mm256_add_ps(va, vb);
        _mm256_store_ps(&result[i], vresult);
    }
}
~~~

Tento kÃ³d provÃ¡dÃ­ souÄet dvou vektorÅ¯ **a** a **b** do vektoru **result** pomocÃ­ AVX vektorovÃ½ch instrukcÃ­. Tyto instrukce provÃ¡dÄ›jÃ­ souÄet osmi ÄÃ­sel souÄasnÄ›, coÅ¾ zvyÅ¡uje rychlost vÃ½poÄtu.

EX2:
NÄ›kdy vÅ¡ak staÄÃ­ do pÅ™ekladaÄe gcc pÅ™idat flag (e.g. *-ftree-vectorizer-verbose=1*), kterÃ½ pÅ™ekladaÄi Å™ekne, Å¾e mÃ¡ sÃ¡m paralelizovat cykly pomocÃ­ vektorovÃ½ch instrukcÃ­. NapÅ™Ã­klad kÃ³d, kterÃ½ jiÅ¾ znÃ¡me:

~~~
for (i = 0; i < 1000; i++) 
	z[i] = x[i] + y[i];
~~~

PoznÃ¡mka: ==ne vÅ¡echny cykly mohou bÃ½t automaticky paralelizovÃ¡ny (napÅ™. v pÅ™Ã­padÄ› chyby ve velikosti poÄÃ¡teÄnÃ­ch a koneÄnÃ½ch vektorovÃ½ch dat)!!!==

#### Procesy a vlÃ¡kna

BÄ›h (bÄ›hovÃ© instance) programu se nazÃ½vÃ¡ **proces**. Procesy jsou nezÃ¡vislÃ© a mohou bÃ½t provÃ¡dÄ›ny paralelnÄ›. Proces obsahuje jedno nebo vÃ­ce **vlÃ¡ken**.

![[Pasted image 20230827174042.png|center|300]]

Procesy Å¾ijÃ­ v navzÃ¡jem oddÄ›lenÃ½ch adresnÃ­ch prostorech! VlÃ¡kna jednoho procesu naopak pamÄ›Å¥ sdÃ­lejÃ­.

![[Pasted image 20230827174237.png|center|400]]

#### GPGPU (General Purpose Graphics Processing Unit)

GPU jsou navrÅ¾eny pro zpracovÃ¡nÃ­ grafickÃ½ch vÃ½poÄtÅ¯, ale lze je takÃ© pouÅ¾Ã­t k provÃ¡dÄ›nÃ­ dalÅ¡Ã­ch operacÃ­! **GPGPU** maximalizuje efektivitu zpracovÃ¡nÃ­ odloÅ¾enÃ­m nÄ›kterÃ½ch operacÃ­ z CPU na GPU. 
KdyÅ¾ nezpracovÃ¡vÃ¡te grafiku, je GPU neustÃ¡le k dispozici pro provÃ¡dÄ›nÃ­ dalÅ¡Ã­ch ÃºkolÅ¯! ProtoÅ¾e GPU jsou optimalizovÃ¡ny pro zpracovÃ¡nÃ­Â vektorÂ vÃ½poÄty, mohou dokonce zpracovat nÄ›kterÃ© instrukce rychleji neÅ¾ CPU.

==V zÃ¡vislosti na tom, na jakÃ© GPGPU se chcete zamÄ›Å™it, mÅ¯Å¾e bÃ½t nutnÃ© pÅ™epnout flag vyuÅ¾itÃ­ GPU!==

EX:

Pro NVIDIA hardwar NVIDIA CUDA Compiler Driver (NVCC): 
*nvcc -mp=gpu -gpu=cc80* (Pro A100 clusteru RCI)

#### Hierarchie cache pamÄ›tÃ­

Hierarchie cache pamÄ›ti v procesoru je systÃ©m, ktery slouÅ¾Ã­ k uklÃ¡dÃ¡nÃ­ Äasto pouÅ¾Ã­vanÃ½ch dat. Tyto pamÄ›ti jsou umÃ­stÄ›ny pÅ™Ã­mo na Äipu procesoru a slouÅ¾Ã­ k urychlenÃ­ pÅ™Ã­stupu k datÅ¯m, kterÃ¡ jsou Äasto pouÅ¾Ã­vÃ¡na programy.

![[Pasted image 20230827182211.png|center|350]]

==PÅ™ed pouÅ¾itÃ­m cache pouÅ¾ijeme pamÄ›Å¥ovÃ© registry, kterÃ© mÃ¡me k dispozici...==

##### L1 Cache

- je prvnÃ­ ÃºroveÅˆ cache pamÄ›ti a je umÃ­stÄ›na pÅ™Ã­mo na jÃ¡dÅ™e procesoru
- mÃ¡ malou kapacitu, ale je extrÃ©mnÄ› rychlÃ¡
- mÃ¡ podkategoriÃ­: 
	- **instrukÄnÃ­ cache (L1i)** uklÃ¡dÃ¡ instrukce (kÃ³dy) programÅ¯, kterÃ© bÄ›Å¾Ã­ na procesoru
	- **datovÃ¡ cache (L1d)** uklÃ¡dÃ¡ data, kterÃ¡ jsou prÃ¡vÄ› pouÅ¾Ã­vÃ¡na programem 

##### L2 Cache

- je druhÃ¡ ÃºroveÅˆ cache pamÄ›ti
- mÃ¡ vÄ›tÅ¡Ã­ kapacitu neÅ¾ L1 cache
- jejÃ­m cÃ­lem je snÃ­Å¾it zÃ¡tÄ›Å¾ na L1 cache a umoÅ¾nit procesoru uklÃ¡dat vÃ­ce dat, kterÃ¡ nebyla nalezena v L1 cache
- mÅ¯Å¾e bÃ½t sdÃ­lena mezi vÃ­ce jÃ¡dry procesoru v multijÃ¡drovÃ½ch procesorech (nebo pokud je to maximÃ¡lnÃ­ ÃºroveÅˆ cache v hierarchii procesoru)

##### L3 cache (Shared LLC)

- je tÅ™etÃ­ ÃºrovnÃ­ cache pamÄ›ti
- sdÃ­lena mezi vÅ¡emi jÃ¡dry procesoru
- mÃ¡ jeÅ¡tÄ› vÄ›tÅ¡Ã­ kapacitu neÅ¾ L2 cache a slouÅ¾Ã­ k uklÃ¡dÃ¡nÃ­ dat a instrukcÃ­, kterÃ© nebyly nalezeny v L1 a L2 cache

##### ProÄ je tu sakra tolik ÃºrovnÃ­?

CÃ­lem je minimalizovat nutnost pÅ™Ã­stupu do hlavnÃ­ pamÄ›ti, kterÃ¡ je pomalejÅ¡Ã­!

---

### Podpora paralelnÃ­ho programovÃ¡nÃ­ v C a C++ 

#### Pthreads (POSIX Threads)

Pthreads (Posix Threads) jsou standardnÃ­ rozhranÃ­ pro vÃ­cevlÃ¡knovÃ© programovÃ¡nÃ­ v operaÄnÃ­m systÃ©mu UNIX a podobnÃ½ch systÃ©mech. PoskytujÃ­ programÃ¡torÅ¯m prostÅ™edky pro vytvÃ¡Å™enÃ­ a sprÃ¡vu vlÃ¡ken (threads) v aplikacÃ­ch.

- pthread_create
- pthread_exit
- pthread_join atd.


#### Thread (VlÃ¡kna v C++)

- vlÃ¡kna jsou exportovÃ¡na hlaviÄkou `<thread>`
- konstruktor vlÃ¡kna zÃ­skÃ¡ prvnÃ­m argumentem funkci, kterou bude vykonÃ¡vat, ostatnÃ­ argumenty pÅ™edÃ¡ volanÃ© funkci
- dÅ™Ã­ve, neÅ¾ je zavolÃ¡n destruktor std::thread, musÃ­me zavolat metodu `join` â€“ ==spouÅ¡tÄ›jÃ­cÃ­ vlÃ¡kno ÄekÃ¡, neÅ¾ spuÅ¡tÄ›nÃ© vlÃ¡kno dobÄ›hne!==

EX:

```
#include <iostream>
#include <thread>

using namespace std; // yes, fuck off

void function(int id, int n)
{
	for (int i = 0; i < n; i++)
	{
		cout << "Vlakno " << id << " rika ahoj!\n";
	}
}

int main()
{
	std::thread t1(function, 1, 2);
	std::thread t2(function, 2, 1)
	t1.join();
	t2.join();
}
```


#### Jthread (Joining threads)

`<jthread>` je novÃ½m pÅ™Ã­davkem v jazyce C++ od standardu C++20. Poskytuje vylepÅ¡enou verzi knihovny `<thread>`: automaticky spravuje ukonÄenÃ­ vlÃ¡kna pÅ™i jeho zniÄenÃ­ (==auto join==)! 

#### Mutex

**Mutex** je struktura pro synchronizaci vlÃ¡ken!
- vlÃ¡kna Å¾Ã¡dajÃ­ o vlastnictvÃ­ (uzamÄenÃ­) mutexu
- mutex mÅ¯Å¾e bÃ½t v danou chvÃ­li uzamÄen pouze jednÃ­m vlÃ¡knem

ZÃ¡mky jsou RAII tÅ™Ã­dy, kterÃ© obsluhujÃ­ uzamykÃ¡nÃ­ a odemykÃ¡nÃ­ mutexÅ¯. Mutexy a zÃ¡mky jsou exportovÃ¡ny hlaviÄkou `<mutex>`.

Problemy? [[#UvÃ¡znutÃ­ (deadlock)]]

EX:

Poznamka: ==`std::unique_lock` volÃ¡ unlock na mutexu v jeho destruktoru!==

```
... 
int counter = 0; 
std::mutex mutex; 

auto thread_func = [&counter, &mutex]() 
{ 
	for (int i = 0; i < 1'000'000; ++i) 
	{ 
		std::unique_lock lock(mutex); 
		counter++; 
		counter--; 
	} // destruktor lock odemkne mutex
}; 
...
```

#### Lock_guard

**std::lock_guard** je tÅ™Ã­da, kterÃ¡ poskytuje automatickou sprÃ¡vu mutexÅ¯. ZajiÅ¡Å¥uje, Å¾e mutex je sprÃ¡vnÄ› uzamknut a odemknut, i kdyÅ¾ dojde k vyhozenÃ­ vÃ½jimky. PouÅ¾itÃ­ `std::lock_guard` je snadnÃ½ zpÅ¯sob, jak nezapomenout na ruÄnÃ­ odemykÃ¡nÃ­ mutexÅ¯ a minimalizovat riziko zaseknutÃ­.

EX:

```
volatile int g_i = 0;
std::mutex g_i_mutex;  // protects g_i
Â 
void safe_increment(int iterations)
{
    const std::lock_guard<std::mutex> lock(g_i_mutex);
    while (iterations-- > 0)
        g_i = g_i + 1;
    // g_i_mutex is automatically released when lock goes out of scope
}
Â 
void unsafe_increment(int iterations)
{
    while (iterations-- > 0)
        g_i = g_i + 1;
}
Â 
int main()
{
    auto test = [](auto func)
    {
        g_i = 0;
        std::jthread t1(1'000'000);
	    std::jthread t2(1'000'000);
	    std::cout << "final g_i: " << g_i << endl;
    };
    
    test(safe_increment);
    test(unsafe_increment);
}
```

#### Atomic

Od verze C++11 existuje vynikajÃ­cÃ­ podpora atomickÃ½ch promÄ›nnÃ½ch v hlaviÄce `<atomic>`. 

**AtomickÃ© promÄ›nnÃ©** jsou alternativa k mutexu nad jednou promÄ›nnou.
- zamknutÃ­ mutexu je pomÄ›rnÄ› nÃ¡kladnÃ¡ operace..
- atomickÃ© operace nejsou zdarma, ale jsou levnÄ›jÅ¡Ã­ 
- pracuje se s nimi podobnÄ› jako s neatomickÃ½mi promÄ›nnÃ½mi, ale operace nad nimi jsou nedÄ›litelnÃ©!

==Pozor, pro synchronizaci nad vÃ­ce promÄ›nnÃ½mi je stÃ¡le potÅ™eba zÃ¡mek!==

1. **VytvoÅ™enÃ­ atomickÃ© promÄ›nnÃ©**: ZaÄnÄ›te tÃ­m, Å¾e vytvoÅ™Ã­te instanci atomickÃ© promÄ›nnÃ© poÅ¾adovanÃ©ho datovÃ©ho typu. NapÅ™Ã­klad `std::atomic<int> counter;`.
2. **ÄŒtenÃ­ hodnoty**: Pro ÄtenÃ­ hodnoty atomickÃ© promÄ›nnÃ© pouÅ¾ijte metodu `load()`. NapÅ™Ã­klad `int value = counter.load();`.
3. **ZÃ¡pis hodnoty**: Pro zÃ¡pis hodnoty do atomickÃ© promÄ›nnÃ© pouÅ¾ijte metodu `store()`. NapÅ™Ã­klad `counter.store(10);`.
4. **Inkrementace hodnoty**: Pro inkrementaci hodnoty atomickÃ© promÄ›nnÃ© mÅ¯Å¾ete pouÅ¾Ã­t metodu `fetch_add()` nebo operÃ¡tor `++`. NapÅ™Ã­klad `counter.fetch_add(1);` nebo `counter++;`.
5. **Dekrementace hodnoty**: Pro dekrementaci hodnoty atomickÃ© promÄ›nnÃ© mÅ¯Å¾ete pouÅ¾Ã­t metodu `fetch_sub()` nebo operÃ¡tor `--`. NapÅ™Ã­klad `counter.fetch_sub(1);` nebo `counter--;`.

EX:

```
#include <iostream>
#include <thread>
#include <atomic>

std::atomic<int> counter(0);

void incrementCounter(int numIncrements) {
    for (int i = 0; i < numIncrements; ++i) {
        counter.fetch_add(1);  // Inkrementace hodnoty atomicky
    }
}

int main() {
    std::thread t1(incrementCounter, 1000000);
    std::thread t2(incrementCounter, 1000000);

    t1.join();
    t2.join();

    std::cout << "Counter value: " << counter.load() << std::endl;

    return 0;
}

```

---

### Komplikace v paralelnÃ­m programovÃ¡nÃ­

#### SoubÄ›h (race condition)

**Race Condition** nastÃ¡vÃ¡, kdyÅ¾ dva nebo vÃ­ce vlÃ¡ken pÅ™istupuje k sdÃ­lenÃ©mu prostÅ™edku (e.g. promÄ›nnÃ¡) zÃ¡roveÅˆ a alespoÅˆ jedno z tÄ›chto vlÃ¡ken provÃ¡dÃ­ zÃ¡pis.

![[Pasted image 20230827205153.png | center | 400]]

#### UvÃ¡znutÃ­ (deadlock)

**Deadlock** je stav ve kterÃ©m program nemÅ¯Å¾e pokraÄovat, protoÅ¾e se rÅ¯znÃ¡ vlÃ¡kna navzÃ¡jem blokujÃ­. K deadlocku obvykle dojde, pokud se vÃ­ce vlÃ¡ken pokusÃ­ zamknout stejnÃ© mutexy v rÅ¯znÃ©m poÅ™adÃ­!

![[Pasted image 20230827200441.png| center | 300]]

![[Pasted image 20230827200922.png|center]]

#### Iluze sdÃ­lenÃ­ (false sharing)

**False sharing** nastÃ¡vÃ¡, kdyÅ¾ nekolik vlÃ¡ken procesÅ¯ pÅ™istupuje k rÅ¯znÃ½m promÄ›nnÃ½m, kterÃ© jsou umÃ­stÄ›ny na stejnÃ©m cache Å™Ã¡dku. I kdyÅ¾ se tyto promÄ›nnÃ© nijak nesdÃ­lÃ­, sdÃ­lenÃ­ cache Å™Ã¡dku zpÅ¯sobuje neefektivnÃ­ komunikaci mezi vlÃ¡kny a sniÅ¾uje vÃ½kon.

![[Pasted image 20230827205420.png | center | 400]]

---

### Podpora paralelnÃ­ho programovÃ¡nÃ­ v OpenMP 

#### SÃ©riovÄ›-paralelnÃ­ model uspoÅ™Ã¡dÃ¡nÃ­ vlÃ¡ken (fork-join)

Model **fork-join**: program se nejprve vykonÃ¡vÃ¡ sekvenÄnÄ› (jedno vlÃ¡kno), a pokud se narazÃ­ na direktivu pro paralelizaci, vlÃ¡kna se "rozdÄ›lÃ­" (fork) a vykonÃ¡vajÃ­ pÅ™idÄ›lenÃ© ÄÃ¡sti kÃ³du paralelnÄ›. Po dokonÄenÃ­ paralelnÃ­ ÄÃ¡sti se vlÃ¡kna "slouÄÃ­" (join) a program pokraÄuje sekvenÄnÄ›.

![[Pasted image 20230828095503.png | center | 400]]

#### ParalelizovatelnÃ¡ Ãºloha (task region)

UmoÅ¾Åˆuje vytvoÅ™enÃ­ paralelnÃ­ho regionu (pomoci [[#Parallel | #pragma omp parallel]]), kde se Ãºlohy mohou vykonÃ¡vat paralelnÄ›. V rÃ¡mci tohoto regionu lze pouÅ¾Ã­t direktivu [[#Task|#pragma omp task]] pro vytvÃ¡Å™enÃ­ samostatnÃ½ch Ãºloh (taskÅ¯), kterÃ© mohou bÃ½t vykonÃ¡vÃ¡ny nezÃ¡visle na sobÄ› vlÃ¡kny v rÃ¡mci paralelnÃ­ho regionu.

#### RÅ¯znÃ© implementace specifikace

#todo

#### Direktivy 

##### Parallel

`#pragma omp parallel` oznaÄuje zaÄÃ¡tek paralelnÃ­ho regionu a vytvÃ¡Å™Ã­ **[[#Threadpool a fronta ÃºkolÅ¯|thread pool]]** (nebo "user-level threads"), kterÃ© budou provÃ¡dÄ›t paralelnÃ­ Ãºlohy. KaÅ¾dÃ© vlÃ¡kno v pool vykonÃ¡vÃ¡ kopii kÃ³du v rÃ¡mci paralelnÃ­ho regionu.
- na zacatek je poÄet vlÃ¡ken nastaven na zÃ¡kladÄ› dostupnÃ©ho hardwaru vlÃ¡ken, ale lze jej ovlivnit promÄ›nnÃ½mi prostÅ™edÃ­ `OMP_NUM_THREADS`, modifikÃ¡torem `num_threads(2)` nebo funkce `omp_set_num_threads()`

##### For

`#pragma omp for` se pouÅ¾Ã­vÃ¡ k paralelnÃ­mu rozdÄ›lenÃ­ iteracÃ­ smyÄky mezi vlÃ¡kna v rÃ¡mci paralelnÃ­ho regionu. KaÅ¾dÃ© vlÃ¡kno dostane pÅ™idÄ›lenou ÄÃ¡st iteracÃ­ a vykonÃ¡ je nezÃ¡visle na sobÄ›.

EX:

```
...
    #pragma omp parallel for
    for (int i = 0; i < vectorSize; ++i) {
        resultVector[i] = vectorA[i] + vectorB[i];
    }

    // VÃ½sledek
    std::cout << "Result vector: ";
    for (int i = 0; i < vectorSize; ++i) {
        std::cout << resultVector[i] << " ";
    }
    return 0;
}
```

##### Section

PomocÃ­ `#pragma omp section` v rÃ¡mci paralelnÃ­ho regionu mÅ¯Å¾eme urÄit, kterÃ½ Ãºsek kÃ³du bude vykonÃ¡vÃ¡n paralelnÄ›.

EX:

- `#pragma omp parallel sections` je k vytvoÅ™enÃ­ paralelnÃ­ho regionu, ve kterÃ©m chceme zpracovat sekce kÃ³du
- kaÅ¾dÃ¡ sekce je oznaÄena `#pragma omp section`

```
int main() {
    #pragma omp parallel sections
    {
        #pragma omp section
        {
            std::cout << "Section A executed by thread: "; 
            std::cout << omp_get_thread_num() << std::endl;
        }

        #pragma omp section
        {
            std::cout << "Section B executed by thread: ";
            std::cout << omp_get_thread_num() << std::endl;
        }
    }
    return 0;
}

```

##### Task

`#pragma omp task` se pouÅ¾Ã­vÃ¡ k vytvoÅ™enÃ­ samostatnÃ© Ãºlohy (tasku) v rÃ¡mci paralelnÃ­ho regionu. Ãšlohy mohou bÃ½t v rÃ¡mci paralelnÃ­ oblasti provÃ¡dÄ›ny vlÃ¡kny nezÃ¡visle.

EX:

- `#pragma omp parallel` pro vytvoÅ™enÃ­ paralelnÃ­ho regionu
- `#pragma omp single` zajistÃ­, Å¾e Ãºkoly budou vytvoÅ™eny pouze jednou, nezÃ¡visle na poÄtu vlÃ¡ken
- `#pragma omp task` k oznaÄenÃ­ Ãºloh
- `#pragma omp taskwait` zajistÃ­, Å¾e se vÅ¡echny Ãºlohy dokonÄÃ­ pÅ™ed pokraÄovÃ¡nÃ­m

```
int main() {
    #pragma omp parallel
    {
        #pragma omp single
        {
            #pragma omp task
            taskA();

            #pragma omp task
            taskB();
            
            #pragma omp taskwait
        }
    }
    return 0;
}
```

##### Barrier

`#pragma omp barrier` slouÅ¾Ã­ k synchronizaci vlÃ¡ken v rÃ¡mci paralelnÃ­ho regionu. PÅ™ed pokraÄovÃ¡nÃ­m dÃ¡le se vlÃ¡kna zastavÃ­ na bariÃ©Å™e, dokud vÅ¡echna vlÃ¡kna nepÅ™ijdou na tento bod.

##### Critical

`#pragma omp critical` zajiÅ¡Å¥uje, Å¾e pouze jedno vlÃ¡kno mÅ¯Å¾e souÄasnÄ› provÃ¡dÄ›t kritickou sekci. TÃ­m se minimalizuje [[#SoubÄ›h (race condition)]] pÅ™i pÅ™Ã­stupu ke sdÃ­lenÃ½m datÅ¯m.

==Jaky je rozil od pouziti atomic promen==?
- `#pragma omp critical`: kaÅ¾dÃ© vlÃ¡kno bude postupnÄ› zÃ­skÃ¡vat a uvolÅˆovat zÃ¡mek, coÅ¾ zajiÅ¡Å¥uje, Å¾e inkrementace bude probÃ­hat atomicky (vÃ½hradnÄ› jedno vlÃ¡kno najednou). To mÅ¯Å¾e zpÅ¯sobit zpomalenÃ­, protoÅ¾e vlÃ¡kna musÃ­ Äekat na zÃ­skÃ¡nÃ­ zÃ¡mku.
- `std::atomic`: atomickÃ© operace jsou navrÅ¾eny tak, aby byly vykonÃ¡vÃ¡ny atomicky bez nutnosti zÃ¡mkÅ¯ nebo kritickÃ½ch sekcÃ­.

EX:

```
int main() {
    int sharedValue = 0;

    #pragma omp parallel
    {
        // KaÅ¾dÃ© vlÃ¡kno pÅ™idÃ¡ hodnotu do sdÃ­lenÃ© promÄ›nnÃ©
        #pragma omp critical
        sharedValue += 1;

        // Synchronizace vÅ¡ech vlÃ¡ken na bariÃ©Å™e
        #pragma omp barrier

        // VÃ½pis sdÃ­lenÃ© hodnoty
        std::cout << "Thread " << omp_get_thread_num();
        std::cout << " sharedValue: " << sharedValue << std::endl;
    }
    return 0;
}
```

##### Atomic

`#pragma omp atomic` se pouÅ¾Ã­vÃ¡ k [[#Atomic|atomickÃ©mu]] provÃ¡dÄ›nÃ­ operacÃ­ nad sdÃ­lenÃ½mi promÄ›nnÃ½mi. ZajiÅ¡Å¥uje, Å¾e operace nad promÄ›nnou se provedou nedÄ›litelnÄ› a minimalizuje [[#SoubÄ›h (race condition)|race condition]].

==Jaky je rozdil atomic promen od omp atomic?==
- `#pragma omp atomic` je specifickÃ¡ direktiva OpenMP, kterÃ¡ funguje v rÃ¡mci kÃ³du paralelizovanÃ©ho pomocÃ­ OpenMP
- `std::atomic` je souÄÃ¡stÃ­ C++ standardnÃ­ knihovny a funguje v celÃ©m kÃ³du, bez ohledu na to, zda je paralelizovÃ¡n pomocÃ­ OpenMP nebo jinÃ½ch technologiÃ­

EX:

```
int main() {
    int counter = 0;

    #pragma omp parallel for
    for (int i = 0; i < 1000000; ++i) {
        #pragma omp atomic
        counter++;
    }
    
    std::cout << "Counter value: " << counter << std::endl;
    return 0;
}
```

---

### Techniky dekompozice programu

#### RozdÄ›lenÃ­ prÃ¡ce

##### StatickÃ© rozdÄ›lenÃ­

PÅ™i **statickÃ©m rozdÄ›lenÃ­ prÃ¡ce** se Ãºlohy pÅ™edem rozdÄ›lÃ­ mezi vlÃ¡kna. KaÅ¾dÃ© vlÃ¡kno dostÃ¡vÃ¡ pevnou ÄÃ¡st Ãºloh a pracuje na nich nezÃ¡visle na ostatnÃ­ch.

- ne nutnÄ› v dobÄ› kompilace
- jednou pÅ™idÄ›lÃ­me vlÃ¡knÅ¯m Ãºkoly a toto pÅ™idÄ›lenÃ­ je nemÄ›nnÃ©
- kdy nÃ¡m statickÃ© rozdÄ›lenÃ­ pomÅ¯Å¾e?
	- vÅ¡echny Ãºkoly trvajÃ­ (pÅ™ibliÅ¾nÄ›) stejnÄ› dlouho
	- kaÅ¾dÃ½ Ãºkol mÅ¯Å¾e trvat rÅ¯znÄ› dlouho, ale vÃ­me pÅ™edem oÄekÃ¡vanou dobu trvÃ¡nÃ­!

![[Pasted image 20230828112251.png|center|400]]

##### DynamickÃ© rozdÄ›lenÃ­

PÅ™i **dynamickÃ©m rozdÄ›lenÃ­ prÃ¡ce** se Ãºlohy rozdÄ›lujÃ­ dynamicky v prÅ¯bÄ›hu vykonÃ¡vÃ¡nÃ­ programu. Ãšlohy jsou pÅ™iÅ™azeny volnÃ½m vlÃ¡knÅ¯m nebo procesÅ¯m podle jejich dostupnosti.

KlÃ­ÄovÃ© cÃ­le:
- [[#BalancovÃ¡nÃ­ a zÃ¡vislosti (dependencies)|VybalancovÃ¡nÃ­]] â€“ aby kaÅ¾dÃ© vlÃ¡kno vykonÃ¡valo (pÅ™ibliÅ¾nÄ›) stejnou prÃ¡ci
- Minimalizace komunikace â€“ aby vlÃ¡kna na sebe nemusely Äekat
- Minimalizace duplicitnÃ­/zbyteÄnÃ© prÃ¡ce â€“ aby vlÃ¡kna nepoÄÃ­tali nÄ›co, co by se nepoÄÃ­talo bez paralelizace
- Program pÅ™idÄ›luje Ãºkoly dynamicky na zÃ¡kladÄ› aktuÃ¡lnÃ­ho vytÃ­Å¾enÃ­ jednotlivÃ½ch vlÃ¡ken -> [[#Threadpool a fronta ÃºkolÅ¯]]

![[Pasted image 20230828112608.png|center|400]]

###### Problemy

Existence " kritickÃ© sekce" v systÃ©mu: vÅ¡echna vlÃ¡kna pouÅ¾Ã­vajÃ­ stejnou frontu!

![[Pasted image 20230828113015.png|center|400]]

##### Threadpool a fronta ÃºkolÅ¯

Co kdyby mÄ›lo kaÅ¾dÃ© vlÃ¡kno vlastnÃ­ frontu?
- MÅ¯Å¾eme vytvoÅ™it vlastnÃ­ frontu pro kaÅ¾dÃ© vlÃ¡kno
- VlÃ¡kno vklÃ¡dÃ¡ a vybÃ­rÃ¡ Ãºkoly z vlastnÃ­ fronty

![[Pasted image 20230828113122.png|center|400]]

Jak zabezpeÄÃ­me [[#BalancovÃ¡nÃ­ a zÃ¡vislosti (dependencies)|vybalancovÃ¡nÃ­]]?
- KdyÅ¾ mÃ¡ vlÃ¡kno prÃ¡zdnou frontu, mÅ¯Å¾e "ukradnout" Ãºkoly z jinÃ© fronty!

![[Pasted image 20230828113222.png|center|400]]

#### BalancovÃ¡nÃ­ a zÃ¡vislosti (dependencies)

IdeÃ¡lnÄ› chceme, aby vÅ¡echna vlÃ¡kna/jÃ¡dra pracovaly a skonÄily souÄasnÄ›.  Pokud jsou nÄ›kterÃ¡ vlÃ¡kna nebo procesy rychlejÅ¡Ã­ neÅ¾ ostatnÃ­, mohou dostÃ¡vat vÃ­ce Ãºloh a vykonÃ¡vat je, zatÃ­mco pomalejÅ¡Ã­ vlÃ¡kna nebo procesy ÄekajÃ­ na dalÅ¡Ã­ Ãºlohy.

![[Pasted image 20230828120152.png|center|500]]

`#pragma omp task` v kombinaci s `depend` a `out` umoÅ¾Åˆuje Å™Ã­dit **zÃ¡vislosti** mezi Ãºlohami v OpenMP. Klauzule `depend` umoÅ¾Åˆuje specifikovat vstupnÃ­ a vÃ½stupnÃ­ zÃ¡vislosti mezi Ãºlohami, zatÃ­mco klauzule `out` urÄuje, kterÃ© hodnoty budou zÃ¡vislÃ©.

EX:

V tomto pÅ™Ã­kladu mÃ¡me dvÄ› Ãºlohy v rÃ¡mci jednoho paralelnÃ­ho bloku:
- PrvnÃ­ Ãºloha vypoÄÃ­tÃ¡ dvojnÃ¡sobek promÄ›nnÃ© `data` a uloÅ¾Ã­ jej do promÄ›nnÃ© `result`. 
  Tato Ãºloha mÃ¡ `out` zÃ¡vislost na promÄ›nnÃ© `result`, coÅ¾ znamenÃ¡, Å¾e dalÅ¡Ã­ Ãºloha, kterÃ¡ bude zÃ¡vislÃ¡ na `result`, nemÅ¯Å¾e zaÄÃ­t, dokud prvnÃ­ Ãºloha neskonÄÃ­.
- DruhÃ¡ Ãºloha vypoÄÃ­tÃ¡ dvojnÃ¡sobek hodnoty v promÄ›nnÃ© `result` a vypÃ­Å¡e jej. 
  Tato Ãºloha mÃ¡ `in` zÃ¡vislost na promÄ›nnÃ© `result`, coÅ¾ znamenÃ¡, Å¾e nemÅ¯Å¾e zaÄÃ­t, dokud prvnÃ­ Ãºloha neukonÄÃ­ svou Äinnost a nedoruÄÃ­ hodnotu `result`.

```
int main() {
    int result = 0;
    int data = 5;

    #pragma omp parallel
    {
        #pragma omp single
        {
            #pragma omp task depend(out:result)  // VÃ½stupnÃ­ zÃ¡vislost: result
            result = data * 2;

            #pragma omp task depend(in:result)   // VstupnÃ­ zÃ¡vislost: result
            {
                int doubledResult = result * 2;
                std::cout << "Doubled result: " << doubledResult << std::endl;
            }
        }
    }

    return 0;
}
```

#### PÅ™Ã­klady z Å™azenÃ­

##### Quick sort

**Quicksort** je algoritmus Å™azenÃ­, kterÃ½ vyuÅ¾Ã­vÃ¡ techniky rozdÄ›l a panuj. Pro dekompozici Quicksortu se pouÅ¾Ã­vÃ¡ nÃ¡sledujÃ­cÃ­ postup:

- Vybere se prvek ze seznamu, kterÃ½ se nazÃ½vÃ¡ pivot.
- Seznam se rozdÄ›lÃ­ na dvÄ› ÄÃ¡sti: jednu s prvky menÅ¡Ã­mi neÅ¾ pivot a druhou s prvky vÄ›tÅ¡Ã­mi.
- ObÄ› ÄÃ¡sti se Å™adÃ­ rekurzivnÄ› pomocÃ­ stejnÃ©ho postupu (rozdÄ›lenÃ­ a Å™azenÃ­).
- VÃ½slednÃ© ÄÃ¡sti se slouÄÃ­ dohromady.

Tento proces je aplikovÃ¡n rekurzivnÄ› na kaÅ¾dou zÃ­skanou ÄÃ¡st, dokud nejsou vÅ¡echny ÄÃ¡sti setÅ™Ã­dÄ›ny. JednÃ¡ se o pÅ™Ã­klad **statickÃ©ho rozdÄ›lenÃ­ prÃ¡ce**, protoÅ¾e kaÅ¾dÃ© rekurzivnÃ­ volÃ¡nÃ­ pracuje s konkrÃ©tnÃ­ ÄÃ¡stÃ­ seznamu!

EX1:

V tomto pÅ™Ã­kladu je paralelizace ÃºspÄ›Å¡nÃ¡ dÃ­ky tomu, Å¾e pokaÅ¾dÃ©, kdyÅ¾ rozdÄ›lÃ­me ÄÃ¡st pole pomocÃ­ pivotu, ponechÃ¡me prvnÃ­ polovinu pro hlavnÃ­ vlÃ¡kno a druhou polovinu pro dalÅ¡Ã­ vlÃ¡kno.

- Klauzuli `shared(vector_to_sort)` ke kaÅ¾dÃ© Ãºloze, abychom sdÃ­leli pole `vector_to_sort` mezi vÅ¡emi Ãºlohami.
- Klauzuli `firstprivate(from, part2)` ke kaÅ¾dÃ© Ãºloze. To znamenÃ¡, Å¾e kaÅ¾dÃ¡ Ãºloha mÃ¡ svou vlastnÃ­ kopii `from` a `part2`. TÃ­m se zajiÅ¡Å¥uje, Å¾e Ãºlohy nebudou sdÃ­let stejnÃ© promene.

![[Pasted image 20230828122642.png|center]]

EX2:

NezapomÃ­nÃ¡me vÅ¡ak na zpÅ¯sob pouÅ¾itÃ­ [[#Task]]. PokusÃ­me se pomocÃ­ nich rozdÄ›lit Ãºlohu na jednotlivÃ© podÃºkoly!

```
void quickSort(std::vector<int>& arr, int low, int high) {
    if (low < high) {
	    // virtualni metoda, ktera vrati odpovidajici cast pole
        int pi = partition(arr, low, high); 

        #pragma omp task
        quickSort(arr, low, pi - 1);
        
        #pragma omp task
        quickSort(arr, pi + 1, high);
    }
}

int main() {
    std::vector<int> data = {12, 11, 13, 5, 6, 7};

    #pragma omp parallel
    {
        #pragma omp single
        quickSort(data, 0, data.size() - 1);
    }
	
	// print array bla-bla
	...
    return 0;
}
```


##### Merge sort

**Mergesort** je takÃ© algoritmus Å™azenÃ­, kterÃ½ pouÅ¾Ã­vÃ¡ techniku rozdÄ›l a panuj. PÅ™i dekompozici Mergesortu se pouÅ¾Ã­vÃ¡ nÃ¡sledujÃ­cÃ­ postup:

- Seznam se rozdÄ›lÃ­ na poloviny, dokud nezbudou jednotlivÃ© prvky nebo podseznamy o velikosti 1.
- JednotlivÃ© prvky nebo podseznamy se nÃ¡slednÄ› slÃ©vajÃ­ dohromady pomocÃ­ procesu merge
- Merge spojuje dvÄ› setÅ™Ã­dÄ›nÃ© ÄÃ¡sti seznamu do jednoho setÅ™Ã­dÄ›nÃ©ho seznamu
  
Postup se opakuje, dokud se seznamy nedoslÃ©vajÃ­ dohromady, a vÃ½sledkem je setÅ™Ã­dÄ›nÃ½ seznam. PÅ™i Mergesortu se v kaÅ¾dÃ©m kroku provÃ¡dÃ­ paralelnÃ­ slÃ©vÃ¡nÃ­ dvou setÅ™Ã­dÄ›nÃ½ch ÄÃ¡stÃ­ seznamu. TÃ­mto zpÅ¯sobem lze vyuÅ¾Ã­t **paralelnÃ­ rozdÄ›lenÃ­ prÃ¡ce** pro rÅ¯znÃ© ÄÃ¡sti seznamu.

EX:

![[Pasted image 20230828125319.png]]

#### PÅ™Ã­klady z numerickÃ© lineÃ¡rnÃ­ algebry a strojovÃ©ho uÄenÃ­

##### NÃ¡sobenÃ­ matice vektorem

Chceme vypoÄÃ­tat vektor $y = Ax$. PÅ™i dekompozici nÃ¡sobenÃ­ matice vektorem se Äasto pouÅ¾Ã­vÃ¡ **paralelnÃ­ rozdÄ›lenÃ­ prÃ¡ce**, kde jednotlivÃ© ÄÃ¡sti vektoru $x$ jsou pÅ™iÅ™azeny k rÅ¯znÃ½m vlÃ¡knÅ¯m. KaÅ¾dÃ© vlÃ¡kno potÃ© vypoÄÃ­tÃ¡ ÄÃ¡st vÃ½slednÃ©ho vektoru $y$, kterÃ¡ odpovÃ­dÃ¡ pÅ™idÄ›lenÃ© ÄÃ¡sti vektoru $x$. Nakonec se vÃ½slednÃ© ÄÃ¡sti slouÄÃ­ dohromady.

EX:

PÅ™edstavme si nÃ¡sledujÃ­cÃ­ matici. Jak je zÅ™ejmÃ©, paralelizace je moÅ¾nÃ¡ buÄ podle Å™Ã¡dkÅ¯, nebo podle sloupcÅ¯. 

![[Pasted image 20230828160051.png|center|400]]

PodÃ­vejme se na prvnÃ­ pÅ™Ã­pad. V zÃ¡vislosti na Å™Ã¡dcÃ­ch budou naÅ¡e vlÃ¡kna provÃ¡dÄ›t poÄÃ­tÃ¡nÃ­ pro kaÅ¾dou buÅˆku finalniho vektoru.  JakÃ© operace by mÄ›la vlÃ¡kna v tomto pÅ™Ã­padÄ› provÃ¡dÄ›t? 
1. Pro kaÅ¾dÃ½ Å™Ã¡dek proveÄte nÃ¡sledujÃ­cÃ­ operace (for)
2. NÃ¡sobenÃ­ kaÅ¾dÃ©ho prvku Å™Ã¡dku kaÅ¾dÃ½m prvkem vektoru (for)
3. SÄÃ­tÃ¡nÃ­ vÅ¡ech souÄinÅ¯ do vÃ½slednÃ© hodnoty polÃ­Äka (for)

ZÃ¡vÄ›r: n-dotazÅ¯ na Å™Ã¡dek + n-dotazÅ¯ na vektor + souÄet
PÅ™i tomto pÅ™Ã­stupu bude obtÃ­Å¾nÃ© algoritmus vektorizovat.
==Pro ÃºspÄ›Å¡nou vektorizaci je dÅ¯leÅ¾itÃ©, aby se pÅ™i provÃ¡dÄ›nÃ­ operace na rÅ¯znÃ½ch prvcÃ­ch matice a vektoru nemusela provÃ¡dÄ›t Å¾Ã¡dnÃ¡ sloÅ¾itÃ¡ podmÃ­nÄ›nÃ¡ vÄ›tvenÃ­.==

![[Pasted image 20230828160434.png|center|400]]

Co kdybychom to zkusili polde sloupcu? VzpomeÅˆme si na pravidlo nÃ¡sobenÃ­ matice vektorem a pÅ™eskoÄme popis vÅ¡ech operacÃ­ a pÅ™ejdÄ›me rovnou k zÃ¡vÄ›ru: n dotazÅ¯ na sloupec, pouze 1 dotaz na vektor a sÄÃ­tÃ¡nÃ­ ve vnÄ›jÅ¡Ã­ smyÄce! To uÅ¾ je jinÃ¡ vÄ›c!

![[Pasted image 20230828160145.png|center|500]]

##### NÃ¡sobenÃ­ dvou matic

1. **RozdÄ›lenÃ­ blokÅ¯**: Matice A a B jsou rozdÄ›leny na menÅ¡Ã­ bloky. Tyto bloky mohou mÃ­t napÅ™Ã­klad velikost 4x4 nebo 8x8 prvkÅ¯. TÃ­m se zajiÅ¡Å¥uje, Å¾e budoucnosti vektorovÃ© instrukce budou moci pracovat s vÄ›tÅ¡Ã­m poÄtem prvkÅ¯ najednou.
2. **ParalelnÃ­ vÃ½poÄet**: Pro kaÅ¾dÃ½ blok matice C (vÃ½sledkovÃ© matice) se provÃ¡dÃ­ vektorizovanÃ½ vÃ½poÄet, kterÃ½ nÃ¡sobÃ­ odpovÃ­dajÃ­cÃ­ bloky matice A a B. Tento krok umoÅ¾Åˆuje vektorovÃ½m instrukcÃ­m provÃ¡dÄ›t nÃ¡sobenÃ­ na vÃ­ce prvcÃ­ch souÄasnÄ›.
3. **SÄÃ­tÃ¡nÃ­ vÃ½sledkÅ¯**: VÃ½sledky nÃ¡sobenÃ­ jednotlivÃ½ch blokÅ¯ jsou nÃ¡slednÄ› seÄÃ­tÃ¡ny do vÃ½sledkovÃ© matice C.

![[Pasted image 20230828163914.png]]

##### Å™eÅ¡enÃ­ systÃ©mu lineÃ¡rnÃ­ch rovnic (GEM)

DalÅ¡Ã­m typickÃ½m Ãºkolem je Å™eÅ¡enÃ­ soustavy lineÃ¡rnÃ­ch rovnic, kde lze vyuÅ¾Ã­t Gaussovu eliminaci!

ZÃ¡vislosti: Po vÃ½bÄ›ru pivota se zmÄ›nÃ­ vÅ¡echny hodnoty pro Å™Ã¡dky a sloupce vÄ›tÅ¡Ã­ neÅ¾ pozice pivota!

#todo

---

## 2. DistribuovanÃ© vÃ½poÄty/systÃ©my

### Ãšvod do distribuovanÃ½ch systÃ©mÅ¯ (DS)

**DS** je soubor (collection) nezÃ¡vislÃ½ch, autonomnÃ­ch vÃ½poÄetnÃ­ch jednotek propojenÃ½ch komunikaÄnÃ­ sÃ­tÃ­. VÃ½poÄetnÃ­ jednotky (nebo *uzly*) komunikujÃ­ formou posÃ­lÃ¡nÃ­ zprÃ¡v za ÃºÄelem urÄitÃ© formy spoluprÃ¡ce.

- Internet
- Blockchain
- Uber
- DatovÃ© centrum
- atd.

Fun fact (haha-momento): Od tÃ© doby, co existujÃ­ poÄÃ­taÄovÃ© sÃ­tÄ› (natoÅ¾ Internet!) uÅ¾ prakticky neexistujÃ­ systÃ©my, kterÃ© by nebyly aspoÅˆ ÄÃ¡steÄnÄ› **distribuovanÃ©**!

![[Pasted image 20230829134744.png|center|400]]

CÃ­le pÅ™i vÃ½voji DS:
- ProblÃ©m **atomickÃ©ho commitu**: atomickÃ½ commit je sada vÃ­ce operacÃ­, kterÃ© jsou provedeny jako jednÃ¡ operace!
  (e.g. **vyberete** si z bankomatu 1000 KÄ -> ze zÅ¯statku z vaÅ¡eho ÃºÄtu **se odeÄte** 1000 KÄ)
- Schopnost Å™eÅ¡it vÃ­ce Ãºloh, neÅ¾ je moÅ¾nÃ© s jednÃ­m poÄÃ­taÄem (podobnÃ© jako u paralelnÃ­ch vÃ½poÄtÅ¯, ale Å™eÅ¡Ã­ i pÅ™Ã­pady, kdyÅ¾ uÅ¾ jeden poÄÃ­taÄ nestaÄÃ­)
- Schopnost zajistit (tÃ©mÄ›Å™) trvalou dostupnost poÅ¾adovanÃ½ch sluÅ¾eb

#### Charakteristiky DS

DistribuovanÃ© systÃ©my | ParalelnÃ­ systÃ©my
--- | ---
<ul><li>VÃ½poÄetnÃ­ jednotky **nemajÃ­ sdÃ­lenou pamÄ›Å¥**</li><li>VÃ½poÄetnÃ­ jednotky **nesdÃ­lejÃ­ globÃ¡lnÃ­ hodiny**</li><li>VÃ½poÄetnÃ­ jednotky **selhÃ¡vajÃ­ nezÃ¡visle**</li></ul> | <ul><li>PÅ™edÃ¡vÃ¡nÃ­ stavu **pouÅ¾Ã­vÃ¡ sdÃ­lenou pamÄ›Å¥** a synchronizaÄnÃ­ mechanismy</li><li>PÅ™Ã­stup ke **spoleÄnÃ½m hodinÃ¡m**</li><li>**SelÅ¾e buÄ nic nebo vÅ¡e**</li></ul>

#### ÄŒas a typy selhÃ¡nÃ­ v DS

PÅ™esnÃ© globÃ¡lnÃ­ hodiny by umoÅ¾nily globÃ¡lnÃ­ uspoÅ™Ã¡dÃ¡nÃ­ vÃ½poÄetnÃ­ch krokÅ¯ v DS. BohuÅ¾el takovÃ© neexistujÃ­ a kaÅ¾dÃ½ proces mÃ¡ svÃ© lokÃ¡lnÃ­ hodiny.
MezitÃ­m lokÃ¡lnÃ­ hodiny nemusÃ­ ukazovat pÅ™esnÃ½ Äas a jejich synchronizace je moÅ¾nÃ¡ jen s urÄitou pÅ™esnostÃ­.

##### AsynchronnÃ­ model DS

Vlastnosti | Schema
--- | :-:
<ul><li>Å½Ã¡dnÃ© ÄasovÃ© limity na rychlost vykonÃ¡vÃ¡nÃ­ procesÅ¯</li><li>Å½Ã¡dnÃ© ÄasovÃ© limity na trvÃ¡nÃ­ pÅ™enosu zprÃ¡v</li><li>Å½Ã¡dnÃ© ÄasovÃ© limity na ÄasovÃ½ drift lokÃ¡lnÃ­ch hodin</li></ul> | ![[Pasted image 20230829142421.png\|250]]

##### SynchronnÃ­ model DS

Vlastnosti | Schema
--- | :-:
<ul><li>ZnÃ¡me hornÃ­ limit na rychlost vykonÃ¡vÃ¡nÃ­ procesÅ¯</li><li>ZnÃ¡mÃ© hornÃ­ limit na dobu pÅ™enosu zprÃ¡v</li><li>Procesy majÃ­ lokÃ¡lnÃ­ hodiny a je znÃ¡m hornÃ­ limit na rychlosti driftu lokÃ¡lnÃ­ch hodin vzhledem k globÃ¡lnÃ­m hodinÃ¡m</li></ul> | ![[Pasted image 20230829142341.png\|250]]

##### ÄŒÃ¡steÄnÄ› synchronnÃ­ model DS

V tomto modelu **mohou procesy zjistit (pomoci timeout), kdyÅ¾ jinÃ© procesy selÅ¾ou**. ProtoÅ¾e se vÅ¡ak detekce provÃ¡dÃ­ pomocÃ­ ÄasovÃ¡nÃ­, vznikajÃ­ nÃ¡klady mnohem vyÅ¡Å¡Ã­ neÅ¾ typickÃ© zpoÅ¾dÄ›nÃ­ zprÃ¡v.

**Timeout? Why tho?** 
Pro vÄ›tÅ¡inu systÃ©mu je relativnÄ› snadnÃ© definovat ÄasovÃ© limity, kterÃ© platÃ­ *vÄ›tÅ¡inu Äasu*. ObÄas se ale mohou vyskytnout obdobÃ­, bÄ›hem kterÃ½ch tyto ÄasovÃ© limity neplatÃ­:

- zpoÅ¾dÄ›nÃ­ procesÅ¯ (e.g. swappovÃ¡nÃ­, garbage collection...) 
- zpoÅ¾dÄ›nÃ­ komunikace (e.g. pÅ™etÃ­Å¾enÃ­ sÃ­tÄ›, ztrÃ¡ta zprÃ¡v...)

##### SelhÃ¡nÃ­ procesu

- **HavÃ¡rie (crash/fail-stop)**: proces pÅ™estane vykonÃ¡vat algoritmus (a reagovat na zprÃ¡vy).
- **LibovolnÃ© (byzantskÃ©) selhÃ¡nÃ­**: proces mÅ¯Å¾e pracovat dÃ¡le (a reagovat na zprÃ¡vy), ale vykonÃ¡vÃ¡ chybnÃ½ algoritmus (z dÅ¯vodu softwarovÃ½ chyby nebo Ãºmyslu).

KrÃ¡tce: nebo chybnÃ½ algoritmus nebo vÅ¯bec bez algoritmu!

##### SelhÃ¡nÃ­ kanÃ¡lu

- **ZtrÃ¡ta zprÃ¡vy (message drop)**: zprÃ¡va nenÃ­ doruÄena cÃ­lovÃ©mu procesu (napÅ™. kvÅ¯li pÅ™etÃ­Å¾enÃ­ sÃ­tÄ›)
- **RozdÄ›lenÃ­ (partitioning)**: procesy jsou rozdÄ›lenÃ© do oddÃ­lÅ¯ (partitions) tak, Å¾e v rÃ¡mci oddÃ­lu je komunikace moÅ¾nÃ¡, ale mezi oddÃ­ly nikoliv

KrÃ¡tce: chyba pÅ™enosu zprÃ¡vy nebo chyba komunikace mezi skupinami procesÅ¯!

##### SelhÃ¡nÃ­ ÄasovÃ¡nÃ­ (timing)

V pÅ™Ã­padÄ› [[#SynchronnÃ­ model DS|synchronnÃ­ch DS]] definujeme jeÅ¡tÄ› selhÃ¡nÃ­ ÄasovÃ¡nÃ­, pokud se doba reakce procesu nebo pÅ™enosu zprÃ¡vy po sÃ­ti liÅ¡Ã­ od pÅ™edem urÄenÃ©ho ÄasovÃ©ho intervalu!

---

### Detekce selhÃ¡nÃ­ v DS

#### ZÃ¡kladnÃ­ protokoly pro detekci selhÃ¡nÃ­

##### CentralizovanÃ½ heartbeat

Princip | Obrazek
--- | :-:
<ul><li>Hearbeats (jako zprÃ¡vy) jsou odesÃ­lÃ¡ny periodicky (kaÅ¾dÃ½ch ğ‘‡ ÄasovÃ½ch jednotek) jednomu vybranÃ©mu procesu</li><li>Hearbeat mÃ¡ poÅ™adovÃ© ÄÃ­slo</li><li>Po odeslÃ¡nÃ­ hearbeatÅ¯ je inkrementovÃ¡n lokÃ¡lnÃ­ counter hearbeatÅ¯ kaÅ¾dÃ©ho procesu</li></ul>| ![[Pasted image 20230829152528.png\|300]]

Klady:
- ÃšplnÃ½ pro vÅ¡echny procesy s vÃ½jimkou $p_j$ (je bezchybnÃ½m procesem)

Zapory:
- SelhÃ¡nÃ­ $p_j$ nenÃ­ detekovÃ¡no... (kdo to bude dÄ›lat? Hmmm???)
- PÅ™i vysokÃ©m poÄtu procesÅ¯ mÅ¯Å¾e bÃ½t $p_j$ pÅ™etÃ­Å¾enÃ½ (overloaded)

##### KruhovÃ½ heartbeat

Princip | Obrazek
--- | :-:
<ul><li>Hearbeats (jako zprÃ¡vy) jsou odesÃ­lÃ¡ny periodicky sousedÅ¯m kaÅ¾dÃ©ho procesu (jedno- nebo oboustrannÄ›)</li></ul>| ![[Pasted image 20230829153345.png\|300]]

Klady:
- NenÃ­ centrÃ¡lnÃ­ bod!

Zapory:
- NeÃºplnÃ© pÅ™i souÄasnÃ©m selhÃ¡nÃ­ vÃ­ce procesÅ¯...
- Je tÅ™eba udrÅ¾ovat kruh (problÃ©m se zasÃ­lÃ¡nÃ­m zprÃ¡v)

##### VÅ¡ichni-vÅ¡em (all-to-all) heartbeat

Princip | Obrazek
--- | :-:
<ul><li>KaÅ¾dÃ½ proces posÃ­lÃ¡ periodicky heartbeats  (jako zprÃ¡vy) vÅ¡em ostatnÃ­m procesÅ¯m</li></ul>| ![[Pasted image 20230829153551.png\|300]]

Klady:
- RovnomÄ›rnÃ¡ zÃ¡tÄ›Å¾ vÅ¡ech procesÅ¯ (ale vysokÃ¡ ÑˆĞ¾ Ğ¿Ğ·Ğ´Ñ†)
- ÃšplnÃ½! Pokud zÅ¯stane aspoÅˆ jeden nehavarovanÃ½ proces, detekuje selhÃ¡nÃ­ libovolnÃ©ho jinÃ©ho procesu.

Zapory:
- NÃ­zkÃ¡ pÅ™esnost.. StaÄÃ­, kdyÅ¾ jeden proces nedostane vÄas heartbeats a mÅ¯Å¾e oznaÄit vÅ¡echny procesy jako havarovanÃ© (vysokÃ© zatÃ­Å¾enÃ­ vÅ¡ech uzlÅ¯ kvÅ¯li neustÃ¡lÃ©mu odesÃ­lÃ¡nÃ­ a pÅ™ijÃ­mÃ¡nÃ­)

#### Detektory selhÃ¡nÃ­ a jejich vlastnosti

PoÅ¾adovanÃ© vlastnosti detektorÅ¯ selhÃ¡nÃ­ jsou:
- **Ãšplnost**: kaÅ¾dÃ© selhÃ¡nÃ­ je v prÅ¯bÄ›hu Äasu detekovÃ¡na alespoÅˆ jednÃ­m bezchybnÃ½m procesem
- **PÅ™esnost**: Å¾Ã¡dnÃ© faleÅ¡nÃ© detekce (pokud je proces detekovÃ¡n jako havarovanÃ½, tak 100% havaroval)
	- chybnÄ› detekovanÃ© selhÃ¡nÃ­ vede ke zbyteÄnÃ½m operacÃ­m, ale lepÅ¡Ã­ neÅ¾ pÅ™ehlÃ©dnutÃ­ havÃ¡rie!
- **Rychlost detekce**: je urÄena Äasem, neÅ¾ prvnÃ­ proces proces detekuje selhÃ¡nÃ­ (ÄÃ­m menÅ¡Ã­, tÃ­m lepÅ¡Ã­)
- **Å kÃ¡lovatelnost**: mÄ›Å™eno poÄtem odeslanÃ½ch zprÃ¡v a rovnomÄ›rnÃ½m rozloÅ¾enÃ­m komunikaÄnÃ­ho zatÃ­Å¾enÃ­

Tyto vlastnosti lze popsat jako faktory, kterÃ© budou mÄ›Å™it efektivitu vytvoÅ™enÃ©ho DS:

![[Pasted image 20230829152319.png | center | 400]]

##### SWIM (Scalable weakly consistent infection-style proces group membership protocol) Detektor SelhÃ¡nÃ­

Princip algoritmu:
1. HlavnÃ­ uzel $p_i$ posÃ­lÃ¡ ostatnÃ­m vlÃ¡knÅ¯m heartbeat zprÃ¡vy (*ping*). ÄŒekÃ¡ se na *timeout*.
2. Pokud nepÅ™ijde Å¾Ã¡dnÃ¡ odpovÄ›Ä (*acknowledge*) z nejakÃ©ho vlÃ¡kna (oznaÄme $p_j$) a dojde k timeoutu, $p_i$ odeÅ¡le poÅ¾adavek na $p_j$-heartbeat (*ping-req*) $k$-nÃ¡hodnÃ½m vlÃ¡knÅ¯m.
3. Tato nÃ¡hodnÃ¡ vlÃ¡kna provedou stejnÃ½ pÅ™Ã­kaz (*ping*) na $p_j$, a pokud $p_j$ odpovÃ­ (*acknowledge*) nebo mlÄÃ­ (*timeout*), poÅ¡lou zprÃ¡vu (*acknowledge*) zpÄ›t na $p_i$.

![[Pasted image 20230829154124.png]]

---

### ÄŒas a kauzalita v DS: UspoÅ™Ã¡dÃ¡nÃ­ udÃ¡lostÃ­ v DS.

#### FyzickÃ© hodiny a jejich synchronizace

ZÃ¡kladnÃ­ myÅ¡lenkou je pouÅ¾itÃ­ **externÃ­ho zdroje synchronizace**!
VÅ¡echny procesy $p_i$ se synchronizujÃ­ s externÃ­m ÄasovÃ½m serverem $S$.

![[Pasted image 20230829162033.png | center]]

Zapory:
- V okamÅ¾iku, kdy $p_i$ obdrÅ¾Ã­ odpovÄ›Ä, se uÅ¾ Äas posunul!
- PÅ™esnost' lokÃ¡lnÃ­ch hodin $C_i$ zÃ¡visÃ­ na komunikaÄnÃ­ latenci!

NeÅ¾ se zaÄneme, je tÅ™eba uvÃ©st nÃ¡sledujÃ­cÃ­ pojmy:

- **MimobÄ›Å¾nost hodin (clock skew)** - RozdÃ­l v Äasu hodin dvou procesÅ¯ (jako vzdÃ¡lenost dvou jedoucÃ­ch automobilÅ¯). 
	- MajÃ­-li dvoje hodiny nenulovou mimobÄ›Å¾nost, jsou nesynchronizovanÃ©.
- **Drift hodin (clock drift)** - RozdÃ­l v rychlosti (frekvenci) hodin dvou procesÅ¯ (jako rozdÃ­l v rychlosti jedoucÃ­ch automobilÅ¯).
	- MajÃ­-li dvoje hodiny nenulovÃ½ drift, tak se jejich mimobÄ›Å¾nost v Äase bude zvyÅ¡ovat.

##### CristianÅ¯v Algoritmus

1. Proces $p_i$ potÅ™ebuje synchronizovat svÅ¯j Äas, zaÅ¡le Å¾Ã¡dost o aktuÃ¡lnÃ­ Äas serveru $S$ v Äase $t$
2. PÅ™i odesÃ­lÃ¡nÃ­ Å¾Ã¡dosti si proces $p_i$ zapamatuje svÅ¯j lokÃ¡lnÃ­ Äas $C_i$
3. Server $S$ obdrÅ¾Ã­ Å¾Ã¡dost od procesu $p_i$ a odeÅ¡le svÅ¯j aktuÃ¡lnÃ­ Äas $t_r$ zpÄ›t k procesu $p_i$
4. Proces $p_i$ vypoÄÃ­tÃ¡ ÄasovÃ½ rozdÃ­l mezi Äasem, kdy Å¾Ã¡dost byla odeslÃ¡na a Äasem, kdy byla pÅ™ijata (tento rozdÃ­l znaÄÃ­, jak dlouho trvala cesta od procesu $p_i$ k serveru $S$)
5. Polovinu tohoto rozdÃ­lu proces $p_i$ pÅ™iÄte k Äasu, kdy byla Å¾Ã¡dost odeslÃ¡na (protoÅ¾e cesta k serveru $S$ a zpÄ›t trvala nÄ›jakou dobu, a tÃ­mto zpÅ¯sobem proces $p_i$ koriguje svÅ¯j Äas tak, aby byl blÃ­Å¾e aktuÃ¡lnÃ­mu Äasu severu $S$) a aktualizuje lokÃ¡lnÃ­ Äas $C_i$!
   ![[Pasted image 20230829173355.png | 250]]

![[Pasted image 20230829173306.png]]

##### NTP

NTP servery jsou uspoÅ™Ã¡dÃ¡ny do stromu, kde:
- uzly synchronizujÃ­ se svÃ½mi rodiÄi a nÄ›kdy i dalÅ¡Ã­mi servery na stejnÃ© Ãºrovni
- klienti tvoÅ™Ã­ listy stromu

![[Pasted image 20230829173955.png | center | 300]]

PÅ™edstavme si nÃ¡sledujÃ­cÃ­ situaci: Potomek (nÄ›jakÃ½ uzel) chce synchronizovat Äas s jednÃ­m ze svÃ½ch rodiÄÅ¯. V takovÃ©m pÅ™Ã­padÄ› je tÅ™eba provÃ©st nÃ¡sledujÃ­cÃ­ kroky:

1. Potomek odeÅ¡le zprÃ¡vu svÃ©mu rodiÄi $Start$ ...
2. #todo 

![[Pasted image 20230829173829.png]]

#### LogickÃ© hodiny a jejich synchronizace

Co kdybychom mÃ­sto absolutnÃ­ho/fyzickÃ©ho Äasu pÅ™iÅ™azovali udÃ¡lostem **logickÃ© ÄasovÃ© znaÄky**? 

Podle novÃ© logiky - udÃ¡losti na sobÄ› zÃ¡visÃ­. Proto pro definovÃ¡nÃ­ tÃ©to zÃ¡vislosti zavÃ¡dÃ­me pojem **[[#KauzÃ¡lnÃ­ zÃ¡vislost/nezÃ¡visost|kauzÃ¡lnÃ­ vztah mezi udÃ¡lostmi]]** (prvnÃ­ udÃ¡lost mÅ¯Å¾e ovlivnit druhou).

SystÃ©m za tÄ›chto podmÃ­nek bude vypadat takto:
- KaÅ¾dÃ½ proces mÃ¡ **stav** (hodnoty promÄ›nnÃ½ch)
- KaÅ¾dÃ½ proces vykonÃ¡vÃ¡ **akce**, aby zmÄ›nil svÅ¯j stav. 
	- Instrukce nebo..
	- PoslÃ¡nÃ­ zprÃ¡vy (send, receive)
- **UdÃ¡lost** je vÃ½skyt akce. PoslÃ¡nÃ­ zprÃ¡vy generuje dvÄ› udÃ¡losti: odeslÃ¡nÃ­ a pÅ™ijetÃ­. 
- KaÅ¾dÃ½ proces mÃ¡ **lokÃ¡lnÃ­ hodiny**

##### KauzÃ¡lnÃ­ zÃ¡vislost/nezÃ¡visost

- **$e_1$ â†’ $e_2$**: **potenciÃ¡lnÄ› kauzÃ¡lnÄ› zÃ¡vislÃ©** udÃ¡losti (tj. $e_1$ mohla ovlivnit $e_2$, ale nemusela)
	- pokud lze relace mezi udÃ¡lostÃ­ popsat jako "stalo se pÅ™ed" â†’ potenciÃ¡lnÃ­ kauzÃ¡lnÃ­ zÃ¡vislost!
	  ![[Pasted image 20230829164907.png|400]]
- **$e_1$ âˆ¥ $e_2$**: **souÄasnÃ©** udÃ¡losti ($e_1$  nemohla ovlivnit $e_2$ a $e_2$ nemohla ovlivnit $e_1$)

EX:

![[Pasted image 20230829165014.png]]

##### LamportÅ¯v Algoritmus

Lamportovy logickÃ© hodiny: 
- KaÅ¾dÃ½ proces mÃ¡ svÃ© logickÃ© hodiny, kterÃ© se synchronizujÃ­ podle pÅ™ijÃ­manÃ­ zprÃ¡v
- PouÅ¾Ã­vanÃ© prakticky ve vÅ¡ech distribuovanÃ½ch systÃ©mech (a vÅ¡ech cloudovÃ½ch platformÃ¡ch)

Princip algoritmu:
KaÅ¾dÃ½ proces $p_i$ si udrÅ¾uje lokÃ¡lnÃ­ logickÃ© hodiny $C_i$ a nastavuje:
1. Po kaÅ¾dÃ© udÃ¡losti, kterÃ¡ se odehraje v $p_i$, se $C_i$ inkrementuje o 1
2. KaÅ¾dÃ© zprÃ¡vÄ› $m$ odeslanÃ© procesem $p_i$ je pÅ™iÅ™azena ÄasovÃ¡ znaÄka $ts(m) = C_i$
3. Kdykoliv proces ğ‘ğ‘— pÅ™ijme zprÃ¡vu $m$, tak provadi nasledujici kroky:
	- upravÃ­ svÃ© lokÃ¡lnÃ­ hodiny $C_j$ na $max\{C_j, ts(m)\}$
	- ==pÅ™ijetÃ­ zprÃ¡vy je udÃ¡lost== a proto provede 1. krok!

![[Pasted image 20230829162521.png]]

---

### GlobÃ¡lnÃ­ stav v DS a jeho vÃ½poÄet

**GlobÃ¡lnÃ­ stav** je mnoÅ¾ina lokÃ¡lnÃ­ stavÅ¯ procesÅ¯ v DS a stavÅ¯ vÅ¡ech komunikaÄnÃ­ch kanÃ¡lÅ¯ v jednom okamÅ¾iku. TakÅ¾e **GlobÃ¡lnÃ­ snapshot** je zÃ¡znamem globÃ¡lnÃ­ stavu!

GlobÃ¡lnÃ­ stav se mÄ›nÃ­, pokud dojde k udÃ¡losti:
- vykonÃ¡nÃ­ instrukce v procesu
- poslÃ¡nÃ­ nebo pÅ™ijetÃ­ zprÃ¡vy

#### ProÄ vÅ¯bec chceme znÃ¡t stav systÃ©mu?

- **Garbage collection** - nutnost identifikovat objekty, na kterÃ© nenÃ­ globÃ¡lnÄ› Å¾Ã¡dnÃ¡ reference
- **Detekce [[#UvÃ¡znutÃ­ (deadlock)]]** - nutnost identifikovat cykly globÃ¡lnÃ­m wait-for grafu
- **Detekce ukonÄenÃ­ vÃ½poÄtu** - nutnost zajistit, Å¾e vÅ¡echny procesy jsou pasivnÃ­ a v Å¾Ã¡dnÃ©m kanÃ¡lu pÅ™enosu nenÃ­ Å¾Ã¡dnÃ¡ zprÃ¡va
- **Checkpointing** za ÃºÄelem obnovenÃ­ globÃ¡lnÃ­ho stavu systÃ©mu.

#### Å˜ez distribuovanÃ©ho vÃ½poÄtu

Pokud bychom mÄ›li globÃ¡lnÃ­ hodiny, tak zaznamenat snapshot by bylo jednoduchÃ©: vÅ¡echny procesy by zaznamenaly stav v dohodnutÃ½ Äas, tj. v jednom fyzickÃ©m okamÅ¾iku. FyzickÃ½ okamÅ¾ik by garantoval konzistenci zaznamenanÃ©ho globÃ¡lnÃ­ho stavu. Jak zaznamenat globÃ¡lnÃ­ snapshotÅ¯ bez globÃ¡lnÃ­ch hodin?

**Å˜ez** je ÄasovÃ¡ hranice v kaÅ¾dÃ©m procesu a v kaÅ¾dÃ©m komunikaÄnÃ­m kanÃ¡le.
- udÃ¡losti, kterÃ© nastanou pÅ™ed Å™ezem, jsou **v Å™ezu**
- udÃ¡losti, kterÃ© nastanou po nÄ›m, jsou **mimo Å™ez**

##### Konzistence

Å˜ez $R$ je **konzistentnÃ­** pokud splÅˆuje [[#KauzÃ¡lnÃ­ zÃ¡vislost/nezÃ¡visost|kauzalitu]], tj. pokud pro kaÅ¾dÃ½ pÃ¡r udÃ¡lostÃ­ $e, f$ v systÃ©mu platÃ­: $f \in R âˆ§ e$ â†’ $f â‡’ e \in R$  (==tj. nelze, aby v Å™ezu byl "dÅ¯sledek" a nebyla tam "pÅ™Ã­Äina"==)

**KonzistentnÃ­ globÃ¡lnÃ­ snapshot** odpovÃ­dÃ¡ konzistentnÃ­mu Å™ezu!
Konzistence Å™ezu je nezÃ¡vislÃ¡ na Äasu (EX2), proto by mÄ›l bÃ½t proveden ve stejnÃ©m fyzickÃ©m Äasu!

EX:

![[Pasted image 20230830161642.png]]

EX2:

![[Pasted image 20230830161813.png]]

#### Chandy-Lamport algoritmus pro distribuovanÃ½ globÃ¡lnÃ­ snapshot

*CÃ­l*: Zaznamenat globÃ¡lnÃ­ snapshot, tj. stav pro kaÅ¾dÃ½ proces a kaÅ¾dÃ½ komunikaÄnÃ­ kanÃ¡l.
*PÅ™edpoklad*: KaÅ¾dÃ½ proces je schopen zaznamenat svÅ¯j vlastnÃ­ stav.
*NovÃ½ pojem*: **ZNAÄŒKA** - SpeciÃ¡lnÃ­ zprÃ¡va, na kterou procesy reagujÃ­.

- pokud $p_i$ dosud nezaznamenal svÅ¯j stav:
	- zaznamenÃ¡ svÅ¯j stav $S_i$
	- zaznamenÃ¡ stav pro kazdy kanÃ¡l $C_{m,i}$ jako prÃ¡zdnou mnoÅ¾inu
	  (==doÂ mnoÅ¾inyÂ budemeÂ uklÃ¡datÂ vÅ¡echnyÂ zprÃ¡vyÂ kterÃ©Â potkÃ¡me,Â protoÅ¾eÂ de-faktoÂ jsouÂ poÂ snapshotu==)
	- odeÅ¡le kaÅ¾dÃ½m odchozÃ­m kanÃ¡lem ZNAÄŒKU (pÅ™edtÃ­m neÅ¾ skrze kanÃ¡l poÅ¡le jakoukoliv jinou zprÃ¡vu)
	- zapne zaznamenÃ¡vÃ¡nÃ­ zprÃ¡v doruÄenÃ½ch skrze vÅ¡echny ostatnÃ­ pÅ™Ã­chozÃ­ kanÃ¡ly (kromÄ› $C_{m,i}$)
- jinak:
	- $p_i$ zaznamenÃ¡ stav kanÃ¡lu $C_{m,i}$ jako mnoÅ¾inu vÅ¡ech zprÃ¡v, kterÃ© $p_i$ obdrÅ¾el skrze $C_{m,i}$ (od inicializaci snapshotu) 
	- $p_i$ ukonÄÃ­ zÃ¡znam kanÃ¡lu $C_{m,i}$

Algoritmus je ukonÄen jakmile:
1. vÅ¡echny procesy zaznamenaly svÅ¯j stav
2. vÅ¡echny procesy pÅ™ijaly ZNAÄŒKU

EX:

1. $p_1$ inicializuji snapshot
	- zaznamenÃ¡ svÅ¯j stav $S_1$
	- zaznamenÃ¡ stav kanÃ¡lu: $C_{2,1}$ (pro $p_2$)
	- zaznamenÃ¡ stav kanÃ¡lu: $C_{3,1}$ (pro $p_3$)
	- odeÅ¡le kaÅ¾dÃ½m odchozÃ­m kanÃ¡lem ($p_2$ a $p_3$) ZNAÄŒKU (a sobe)
2. $p_3$ pÅ™ijmÃ¡ ZNAÄŒKU od $p_1$
	- zaznamenÃ¡ svÅ¯j stav $S_3$
	- zaznamenÃ¡ stav kanÃ¡lu $C_{1,3} = \{\}$ (jako prazdna mnozina, kde budou doruÄenÃ© zprÃ¡vy, kterÃ© pÅ™iÅ¡ly po tÃ©to ZNAÄŒCE)
	- zaznamenÃ¡ stav kanÃ¡lu $C_{2,3}$ (pro $p_2$)
	- odeÅ¡le kaÅ¾dÃ½m odchozÃ­m kanÃ¡lem ($p_1$ a $p_2$) ZNAÄŒKU
3. $p_1$  pÅ™ijmÃ¡ ZNAÄŒKU od $p_3$
	- zaznamenÃ¡ stav kanÃ¡lu $C_{3,1} = \{\}$ (jako prazdna mnozina, kde budou doruÄenÃ© zprÃ¡vy, kterÃ© pÅ™iÅ¡ly po tÃ©to ZNAÄŒCE)
	- ukonÄÃ­ zÃ¡znam kanÃ¡lu $C_{3,1}$
4. $p_2$ pÅ™ijmÃ¡ ZNAÄŒKU od $p_3$
	- zaznamenÃ¡ svÅ¯j stav $S_2$
	- zaznamenÃ¡ stav kanÃ¡lu $C_{3,2} = \{\}$ (jako prazdna mnozina, kde budou doruÄenÃ© zprÃ¡vy, kterÃ© pÅ™iÅ¡ly po tÃ©to ZNAÄŒCE)
	- zaznamenÃ¡ stav kanÃ¡lu $C_{1,2}$ (pro $p_1$)
	- odeÅ¡le kaÅ¾dÃ½m odchozÃ­m kanÃ¡lem ($p_1$ a $p_3$) ZNAÄŒKU
5. $p_2$ pÅ™ijmÃ¡ ZNAÄŒKU od $p_1$
	- zaznamenÃ¡ stav kanÃ¡lu $C_{1,2} = \{\}$ (jako prazdna mnozina, kde budou doruÄenÃ© zprÃ¡vy, kterÃ© pÅ™iÅ¡ly po tÃ©to ZNAÄŒCE)
	- ukonÄÃ­ zÃ¡znam kanÃ¡lu $C_{1,2}$
6. $p_3$ pÅ™ijmÃ¡ ZNAÄŒKU od $p_2$
	- zaznamenÃ¡ stav kanÃ¡lu $C_{2,3} = \{\}$ (jako prazdna mnozina, kde budou doruÄenÃ© zprÃ¡vy, kterÃ© pÅ™iÅ¡ly po tÃ©to ZNAÄŒCE)
	- ukonÄÃ­ zÃ¡znam kanÃ¡lu $C_{2,3}$
7. $p_1$  pÅ™ijmÃ¡ ZNAÄŒKU od $p_2$
	- zaznamenÃ¡ stav kanÃ¡lu $C_{2,1} = \{H â†’ D\}$ (H â†’ D je po ZNAÄŒCE, proto ukladame)
	- ukonÄÃ­ zÃ¡znam kanÃ¡lu $C_{2,1}$
8. KonzistentnÃ­ Å˜ez bude podle vÅ¡ech stavu: $S_1$, $S_2$, $S_3$

![[Pasted image 20230830165024.png]]
![[Pasted image 20230830174034.png]]

#### StabilnÃ­ vlastnosti DS

VÃ½sledkem Chandy-Lamport algoritmu pro vÃ½poÄet globÃ¡lnÃ­ snapshotu je [[#Konzistence|konzistentnÃ­ Å™ez]].
Chandy-Lamport snapshotÅ¯ algoritmus lze pouÅ¾Ã­t pro detekci stabilnÃ­ch globÃ¡lnÃ­ch vlastnostÃ­.

##### Å½ivost (Liveness)

Garance, Å¾e v DS Äasem **dojde k nÄ›Äemu dobrÃ©mu** (bude dosaÅ¾en Å¾Ã¡doucÃ­ stav).

PÅ™Ã­klady:
- DistribuovanÃ½ vÃ½poÄet -> vÃ½poÄet skonÄÃ­
- Detekce selhÃ¡nÃ­ -> kaÅ¾dÃ© selhÃ¡nÃ­ je detekovÃ¡no
- GlobÃ¡lnÃ­ snapshot -> algoritmus dobÄ›hne (lol)

##### BezpeÄnost (Safety)

Garance, Å¾e v DS nikdy **nedojde k nÄ›Äemu Å¡patnÃ©mu** (nebude dosaÅ¾en neÅ¾Ã¡doucÃ­ stav). PÅ™Ã­klady: 
- Nedojde k [[#UvÃ¡znutÃ­ (deadlock)|deadlocku]]
- Detekce selhÃ¡nÃ­ -> pÅ™esnost detekci
- GlobÃ¡lnÃ­ snapshot -> snapshot je konzistentnÃ­m Å™ezem

##### StabilnÃ­ vlastnost

Je takovÃ¡ vlastnost, Å¾e **jakmile je ve vÃ½poÄtu jednou splnÄ›na, zÅ¯stÃ¡vÃ¡ splnÄ›na navÅ¾dy**.

- pÅ™Ã­klad stabilnÃ­ vlastnost [[#Å½ivost (Liveness)|Å¾ivosti]]: vÃ½poÄet skonÄil
- pÅ™Ã­klad stabilnÃ­ vlastnosti *poruÅ¡ujÃ­cÃ­* [[#BezpeÄnost (Safety)|bezpeÄnost]]: nastalo uvÃ¡znutÃ­
- pÅ™Ã­klad nestabilnÃ­ vlastnosti: na vstup do kritickÃ© sekce ÄekajÃ­ prÃ¡vÄ› dva procesy (deadlock)...

---

### VzÃ¡jemnÃ© vylouÄenÃ­ procesÅ¯ v DS

**KritickÃ¡ sekce (KS)** je ÄÃ¡st kÃ³du (vÅ¡ech procesÅ¯), u kterÃ© potÅ™ebujeme zaruÄit, Å¾e ji vykonÃ¡vÃ¡ v kaÅ¾dÃ©m okamÅ¾iku maximÃ¡lnÄ› jeden proces.

PoÅ¾adavky na algoritmus pro vylouÄenÃ­ procesu:
- BezpeÄnost: nejvÃ½Å¡e jeden proces v kritickÃ© sekci v okamÅ¾iku
- Å½ivost: kaÅ¾dÃ½ poÅ¾adavek na vstup do kritickÃ© sekce je Äasem uspokojen

#### CentralizovanÃ½ algoritmus

- ZvolÃ­me koordinÃ¡tora (pomocÃ­ algoritmu [[#Volba lÃ­dra v DS|volby lÃ­dra]])
- LÃ­dr spravuje speciÃ¡lnÃ­ **token** (umoÅ¾Åˆuje drÅ¾iteli vstup do KS) a **frontu poÅ¾adavkÅ¯** na vstup do KS

Princip:
1. LibovolnÃ½ proces $p$ poÅ¡li poÅ¾adavek lÃ­drovi
2. Po pÅ™ijetÃ­ poÅ¾adavku z procesu $p$:
	- lÃ­dr mÃ¡ **token** ? -> pÅ™edÃ¡ **token** procesu $p$
	- lÃ­dr nemÃ¡ **token** ? -> pÅ™edÃ¡ proces $p$ do fronty
3. Po pÅ™ijetÃ­ **tokenu** od procesu $p$:
	- **fronta** je prÃ¡zdnÃ¡ ? -> uchovÃ¡ **token**
	- **fronta** nenÃ­ prÃ¡zdnÃ¡ ? -> **fronta.pop()** proces z hlavy a poÅ¡lÃ­ mu **token**

![[Pasted image 20230830230836.png|center|500]]

AnalÃ½za:
- BezpeÄnost: MaximÃ¡lnÄ› jeden proces v KS
- Å½ivost: na kaÅ¾dÃ½ poÅ¾adavek Äasem dojde

#### KruhovÃ½ algoritmus

EZ-PZ algoritmus. $N$ procesÅ¯ organizovanÃ½ch do kruhu.
- KaÅ¾dÃ½ proces mÅ¯Å¾e poslat zprÃ¡vu svÃ©mu nÃ¡slednÃ­kovi
- Koluje prÃ¡vÄ› jeden **token**

Princip:
1. Proces $p$ ceka dokud nedostane **token**
2. Proces $p$ ma **token** ? -> vstup do KS
   (pokud obdrÅ¾Ã­ **token** a neni v KS, tak pÅ™eda **token** nÃ¡slednÃ­kovi)
3. vystup z KS ? -> Proces $p$ posli **token** nÃ¡slednÃ­kovi

![[Pasted image 20230830232722.png | center | 400]]

AnalÃ½za:
- BezpeÄnost: prÃ¡vÄ› jeden **token**
- Å½ivost: **token** Äasem obÄ›hnou celÃ½ kruh

#### Ricart-AgrawalÅ¯v algoritmus

NepouÅ¾Ã­vÃ¡ token, ale vyuÅ¾Ã­vÃ¡ [[#KauzÃ¡lnÃ­ zÃ¡vislost/nezÃ¡visost|kauzalitu]] (skalÃ¡rnÃ­ hodiny).
- KaÅ¾dÃ½ proces si udrÅ¾uje logickou promÄ›nou stav (inicializovanou na RELEASED) a seznam poÅ¾adavkÅ¯ na vstup

Princip:
1. Proces $P_i$ chci do KS:
	- NastavÃ­ stav na **WANTED** âŸ¨ğ‘‡ğ‘– , iâŸ©
	- PoÅ¡li **REQUEST** âŸ¨ğ‘‡ğ‘– , iâŸ© vÅ¡em procesÅ¯m (âŸ¨ğ‘‡ğ‘– , iâŸ© je aktuÃ¡lnÃ­ LamportÅ¯v logickÃ½ Äas v $P_i$)
2. Proces $P_j$ po pÅ™ijetÃ­ REQUEST:
	- stav = **HELD** ? -> pÅ™idava **REQUEST** do seznamu ÄekajÃ­cÃ­ch poÅ¾adavkÅ¯
	- stav = **WANTED** a lokalni LamportÅ¯v cas je vetsi nez LamportÅ¯v cas z pozadavku -> pÅ™idava **REQUEST** do seznamu ÄekajÃ­cÃ­ch poÅ¾adavkÅ¯
	- jinak -> poÅ¡li **OK** do $P_i$
3. Proces $P_i$ po uvolneni KS:
	- stav na **RELEASED**
	- poÅ¡li **OK** vÅ¡em ÄekajÃ­cÃ­m procesÅ¯m ze seznamu

EX:

Krok | Dalsi krok
:-: | :-:
1.![[Pasted image 20230831010528.png\|330]] | 2.![[Pasted image 20230831005853.png\|330]]
3.![[Pasted image 20230831005911.png\|330]] | 4.![[Pasted image 20230831005943.png\|330]]
5.![[Pasted image 20230831010009.png\|330]] | 6.![[Pasted image 20230831010057.png\|330]]
7.![[Pasted image 20230831010107.png\|330]] | 8.![[Pasted image 20230831010124.png\|330]]

AnalÃ½za:
- Å½ivost: nejhorÅ¡Ã­ pÅ™Ã­pad â€“ je potÅ™eba poÄkat neÅ¾ vÅ¡ech $N-1$ poÅ¡le OK 
- BezpeÄnost: Dva procesy nemohou souÄasnÄ› zÃ­skat pÅ™Ã­stup do KS

#### MaekawÅ¯v algoritmus

#todo 

### Volba lÃ­dra v DS

Problem: Ze skupiny procesÅ¯ **vybrat lÃ­dra** (kterÃ½ bude Å™eÅ¡it specifickÃ© Ãºkoly) a **dÃ¡t vÄ›dÄ›t vÅ¡em procesÅ¯m** ve skupinÄ›, kdo je lÃ­drem.

#### KruhovÃ½ algoritmus
Neco podobneho jako [[#KruhovÃ½ algoritmus]] pro vstup do KS.

- Procesy jsou uspoÅ™Ã¡danÃ© do logickÃ©ho kruhu. 
- ZprÃ¡vy jsou posÃ­lÃ¡ny dokola kruhu v jednom smÄ›ru. 
- Procesy jsou schopnÃ© detekovat selhÃ¡nÃ­ ostatnÃ­ch procesÅ¯

Princip:
1. $P_i$ poÅ¡le po kruhu zprÃ¡vu ELECTION($i$) obsahujÃ­cÃ­ jeho identifikÃ¡tor
2. Process zpracovÃ¡ ELECTION($j$):
	- i < j ? -> pÅ™epoÅ¡le zprÃ¡vu ELECTION($j$)
	- i > j ? -> nahradÃ­ zprÃ¡vu za ELECTION($i$) a poÅ¡le dÃ¡l
	- i = j ? -> $P_i$ je novÃ½ koordinÃ¡tor; odeÅ¡le zprÃ¡vu ELECTED($i$)

![[Pasted image 20230901134552.png|center|350]]

3. Process zpracovÃ¡ ELECTED($j$):
	- NastavÃ­ svou promÄ›nou `elected â‰” j` 
	- i â‰  j ? -> pÅ™epoÅ¡le zprÃ¡vu ELECTED($j$)

![[Pasted image 20230901134720.png|center|450]]

Jak Å™eÅ¡it selhÃ¡nÃ­?

PÅ™edchÅ¯dce nebo nÃ¡sledovnÃ­k rÃ¡doby lÃ­dra $P_k$ detekuje selhÃ¡nÃ­ a spustÃ­ novÃ© volby:
- Pokud obdrÅ¾Ã­ zprÃ¡vu ELECTION(k), ale neobdrÅ¾Ã­ odpovÃ­dajÃ­cÃ­ zprÃ¡vu ELECTED(k)
- Pokud obdrÅ¾Ã­ podruhÃ© zprÃ¡vu ELECTION(k) nebo ELECTED(k)

#### Algoritmus Bully

ZÃ¡kladnÃ­ princip: proces, kterÃ½ detekoval selhÃ¡nÃ­ lÃ­dra, vyzve procesy s vyÅ¡Å¡Ã­m ID ve volbÃ¡ch.

Princip:
1. (detekce selhÃ¡nÃ­, volby!) Proces $P_i$ poÅ¡li ELECTION zprÃ¡vu vÅ¡em procesÅ¯m s vyÅ¡Å¡Ã­m ID
	- Pozor: pokud $P_i$ mÃ¡ nejvyÅ¡Å¡Ã­ ID poÅ¡li COORDINATOR zprÃ¡vu vÅ¡em procesÅ¯m s niÅ¾Å¡Ã­mi identifikÃ¡tory (volby konÄÃ­)

![[Pasted image 20230901135857.png|center|400]]

2. Proces zpracovÃ¡ ELECTION:
	- Posyla OK (volby v Rusku haha)
	- pokud $P_i$ dosud nezahÃ¡jil volby zahaj volby! (krok 1., rekurzivne volby, aby vsechny procesy se zucastnili, kdo je vys podle ID)

![[Pasted image 20230901135949.png|center|400]]

3. Proces ÄekÃ¡ na odpovÄ›di (po vyvolÃ¡nÃ­ voleb):
	- pokud nedorazÃ­ Å¾Ã¡dnÃ¡ odpovÄ›Ä v ÄasovÃ©m limitu T -> prohlaÅ¡ se jako $P_i$ za lÃ­dra a poÅ¡li COORDINATOR zprÃ¡vu vÅ¡em procesÅ¯m s niÅ¾Å¡Ã­m ID
	- odpoved je ? -> existuje aktivnÃ­ proces s vyÅ¡Å¡Ã­m ID, Äekej na zprÃ¡vu COORDINATOR !

![[Pasted image 20230901140030.png|center|400]]

---

### Konsensus v DS

**Konsensus** v DS se tÃ½kÃ¡ procesu dosaÅ¾enÃ­ shody mezi rÅ¯znÃ½mi procesy na spoleÄnÃ© hodnotÄ› nebo rozhodnutÃ­. CÃ­lem je, aby vÅ¡echny sprÃ¡vnÄ› fungujÃ­cÃ­ procesy v systÃ©mu dosÃ¡hly shody a pÅ™ijaly stejnou hodnotu.

#### FLP teorÃ©m

**FLP teorÃ©m** (Fischer, Lynch, Paterson) je teorÃ©m, kterÃ½ se tÃ½kÃ¡ konsensu v asynchronnÃ­ch DS. Tento teorÃ©m tvrdÃ­, Å¾e v asynchronnÃ­m systÃ©mu, ve kterÃ©m existuje alespoÅˆ jeden selhÃ¡vajÃ­cÃ­ proces, nelze dosÃ¡hnout konsensu v libovolnÃ©m Äase!

Kratce: ==Algoritmus pro konsensus musÃ­ garantovat bezpeÄnost a mÄ›l by maximalizovat dostupnost. ObojÃ­ garantovat nelze!==

#### RAFT

![[Pasted image 20230902104926.png]]

- **UdrÅ¾ovÃ¡nÃ­ Å¾ivosti (Heartbeats)**
	1. LÃ­dr pravidelnÄ› odesÃ­lÃ¡ zprÃ¡vy "heartbeat" ostatnÃ­m uzlÅ¯m, aby ukÃ¡zal, Å¾e je stÃ¡le aktivnÃ­
	2. Uzly odpovÃ­dajÃ­ na heartbeat zprÃ¡vy, aby potvrdily svou dostupnost.

- **Volba lÃ­dra (Leader Election)**
    1. VÃ½chozÃ­ stav systÃ©mu je, Å¾e vÅ¡ichni uzly jsou ve stavu FOLLOWER
    2. Pokud je lÃ­dr nedostupnÃ½ (mrtvÃ½ nebo vÃ½padek sÃ­tÄ›), uzel mÅ¯Å¾e vyhlÃ¡sit volbu lÃ­dra!
    3. KaÅ¾dÃ½ uzel poÅ¡le zprÃ¡vu REQUEST_VOTE ostatnÃ­m uzlÅ¯m a Å¾Ã¡dÃ¡ je o hlas
    4. Uzel hlasuje pro sebe a posÃ­lÃ¡ zprÃ¡vy ostatnÃ­m uzlÅ¯m
    5. Uzel, kterÃ½ obdrÅ¾Ã­ vÄ›tÅ¡inu hlasÅ¯, se stÃ¡vÃ¡ lÃ­drem.

- **Neutralizace starÃ½ch lÃ­drÅ¯**
    1. KaÅ¾dÃ½ uzel v systÃ©mu pravidelnÄ› oÄekÃ¡vÃ¡ "heartbeat" od aktuÃ¡lnÃ­ho lÃ­dra
    2. Pokud uzel neobdrÅ¾Ã­ "heartbeat" od lÃ­dra po urÄitou dobu, povaÅ¾uje lÃ­dra za nedostupnÃ©ho nebo selhalÃ©ho -> VOLBY!!!

![[Pasted image 20230902105159.png]]

- **Replikace logÅ¯ (Log Replication)**
	1. LÃ­dr je zodpovÄ›dnÃ½ za koordinaci replikace logÅ¯ mezi uzly
	2. KaÅ¾dÃ½ uzel v systÃ©mu udrÅ¾uje svÅ¯j vlastnÃ­ log operacÃ­
	3. KdyÅ¾ klient poÅ¡le lÃ­drovi poÅ¾adavek na provedenÃ­ operace, lÃ­dr zapisuje tento poÅ¾adavek do svÃ©ho logu (append-only) a replikuje ho na ostatnÃ­ uzly
	4. Kdykoli je pÅ™ijat novÃ½ poÅ¾adavek, je do logu pÅ™idan s aktuÃ¡lnÃ­m ÄÃ­slem TERM (epocha)

![[Pasted image 20230902105303.png]]

- **ZpracovÃ¡nÃ­ poÅ¾adavkÅ¯ (Request Processing)**
	1. KaÅ¾dÃ½ uzel mÅ¯Å¾e pÅ™ijÃ­mat poÅ¾adavky od klientÅ¯ a pÅ™edÃ¡vat je lÃ­drovi
	2. LÃ­dr zpracovÃ¡vÃ¡ poÅ¾adavky od klientÅ¯ a replikuje je na ostatnÃ­ uzly (krok vys) 

![[Pasted image 20230901142344.png]]